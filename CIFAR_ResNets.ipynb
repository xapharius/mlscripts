{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR ResNets\n",
    "- I couldn't find a unified factory for ResNets that combines Imagenet and CIFAR model designs, so here is the cleanest version I came up with.\n",
    "- Avoiding rewriting boilerplate -> patching torchvision models to fit CIFAR specification.\n",
    "    - In order to make Imagenet models CIFAR compatible: replace the first conv, remove the maxpooling and adjust the fc layer.\n",
    "    - To build CIFAR-native models: instantiate an Imagenet model and rebuild the layers to specification, a lot cleaner than rewriting the _make_layers.\n",
    "- Comparing to Akamaster implementation: Same output, Same training perf\n",
    "- Benchmarking models on CIFAR10/CIFAR100\n",
    "- Benchmarking training strategies/ batch size for ResNet56 on CIFAR100\n",
    "- Other notes:\n",
    "    - Optimisation (opt + scheduler) and batch_size are the major hyper-parameters\n",
    "    - mixed-precision, num_workers, deterministic/benchmarking don't impact acc results in pytorch 2\n",
    "    - Small Arch changes, initialisation, not very impactful\n",
    "    - Very difficult to get Resnet56 to 73% on CIFAR100 - only with bs=64 and a 200-epoch optimsation\n",
    "\n",
    "\n",
    "## step-200 bs=64\n",
    "\n",
    "| ds/model   | cifar10 | cifar100 |\n",
    "|-----------|---------|----------|\n",
    "| resnet18  | 0.95    | 0.78     |\n",
    "| resnet20  | 0.92    | 0.68     |\n",
    "| resnet32  | 0.93    | 0.69     |\n",
    "| resnet34  | 0.96    | 0.79     |\n",
    "| resnet44  | 0.92    | 0.70     |\n",
    "| resnet50  | 0.95    | 0.78     |\n",
    "| resnet56  | 0.92    | 0.71     |\n",
    "\n",
    "## step-160 bs=256\n",
    "\n",
    "| ds/model   | cifar10 | cifar100 |\n",
    "|------------|---------|----------|\n",
    "| resnet18   | 0.92    | 0.71     |\n",
    "| resnet20   | 0.91    | 0.67     |\n",
    "| resnet32   | 0.91    | 0.67     |\n",
    "| resnet34   | 0.92    | 0.67     |\n",
    "| resnet44   | 0.92    | 0.68     |\n",
    "| resnet50   | 0.91    | 0.65     |\n",
    "| resnet56   | 0.89    | 0.68     |\n",
    "\n",
    "## ResNet56 CIFAR100\n",
    "\n",
    "| bs / opt | 64     | 128    | 256    |\n",
    "|------------|--------|--------|--------|\n",
    "| cos-200    | 0.7272 | 0.7162 | 0.7147 |\n",
    "| step-160   | 0.6997 | 0.6861 | 0.6740 |\n",
    "| step-200   | 0.7127 | 0.7107 | 0.7117 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet\n",
    "\n",
    "CIFAR_MODELS = [\"resnet20\", \"resnet32\", \"resnet44\", \"resnet56\", \"resnet110\", \"resnet1202\"]\n",
    "IMAGENET_MODELS = [\"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\", \"resnet152\"]\n",
    "\n",
    "\n",
    "class ShortcutA(nn.Module):\n",
    "    \"\"\"Option A: downsample via stride 2 and zero pad for extra channels\"\"\"\n",
    "    def __init__(self, out_channels):\n",
    "        super(ShortcutA, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        missing = self.out_channels - x.size(1)\n",
    "        return F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, missing//2, missing//2), \"constant\", 0)\n",
    "\n",
    "\n",
    "def get_resnet(model: str, num_classes: int):\n",
    "    \"\"\"\n",
    "    :author: @xapharius\n",
    "    ResNet factory for CIFAR compatible models. Supports native CIFAR models and patched Imagenet models.\n",
    "    Weights are not initialised.\n",
    "    CIFAR Models: resnet{20, 32, 44, 56, 110, 1202}\n",
    "    Imagenet Models: resnet{18, 34, 50, 101, 152}\n",
    "    \"\"\"\n",
    "    if model in IMAGENET_MODELS:\n",
    "        model = resnet.__dict__[model]()\n",
    "        model.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False)\n",
    "        model.maxpool = nn.Identity()\n",
    "        model.fc = nn.Linear(512 * model.layer1[0].expansion, num_classes)\n",
    "        return model\n",
    "    if model in CIFAR_MODELS:\n",
    "        n_blocks = (int(model.replace(\"resnet\", \"\")) - 2) // 6\n",
    "        model = resnet.resnet18() # layers will be replaced\n",
    "        model.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1, bias=False)\n",
    "        model.bn1 = nn.BatchNorm2d(16)\n",
    "        model.maxpool = nn.Identity()\n",
    "        model.inplanes = 16\n",
    "        model.layer1 = model._make_layer(block=resnet.BasicBlock, planes=16, blocks=n_blocks, stride=1)\n",
    "        model.layer2 = model._make_layer(block=resnet.BasicBlock, planes=32, blocks=n_blocks, stride=2)\n",
    "        model.layer2[0].downsample = ShortcutA(32) \n",
    "        model.layer3 = model._make_layer(block=resnet.BasicBlock, planes=64, blocks=n_blocks, stride=2)\n",
    "        model.layer3[0].downsample = ShortcutA(64)\n",
    "        model.layer4 = nn.Identity()\n",
    "        model.fc = nn.Linear(64, num_classes)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "- Using standard crop+flip augmentation\n",
    "- In the literature there are 3 main optimisation strategies:\n",
    "    - MultiStep 160 - milestones: [80, 120], gamma: 0.1\n",
    "    - MultiStep 200 - milestones: [60, 120, 250], gamma: 0.2\n",
    "    - CosineAnnealing 200\n",
    "- Major hyper-parameters are\n",
    "    - Optimisation strategy\n",
    "    - Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning as L\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "\n",
    "NORMVALS = {\n",
    "    \"mean\": {\n",
    "        \"cifar10\": [0.4914, 0.4822, 0.4465],\n",
    "        \"cifar100\": [0.5071, 0.4867, 0.4408],\n",
    "    },\n",
    "    \"std\": {\n",
    "        \"cifar10\": [0.2023, 0.1994, 0.2010],\n",
    "        \"cifar100\": [0.2675, 0.2565, 0.2761],\n",
    "    },\n",
    "}\n",
    "\n",
    "DATASET_ROOT = \"/data/datasets\"\n",
    "\n",
    "\n",
    "def get_datamodule(dataset: str, batch_size=256, num_workers=10):\n",
    "    ds_cls = CIFAR10 if dataset == \"cifar10\" else CIFAR100\n",
    "    ds_path = DATASET_ROOT + \"/\" + dataset.upper()\n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(NORMVALS[\"mean\"][dataset], NORMVALS[\"std\"][dataset]),\n",
    "        ]\n",
    "    )\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(NORMVALS[\"mean\"][dataset], NORMVALS[\"std\"][dataset]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_ds = ds_cls(root=ds_path, transform=train_transform, train=True)\n",
    "    test_ds = ds_cls(root=ds_path, transform=test_transform, train=False)\n",
    "\n",
    "    dm = L.LightningDataModule.from_datasets(train_ds, val_dataset=test_ds, batch_size=batch_size, num_workers=num_workers)\n",
    "    return dm\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class LitModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, model: nn.Module, opt_strat=\"step-200\"):\n",
    "        super().__init__()\n",
    "        self.model = model.apply(init_weights)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        num_classes = [m for m in model.modules() if isinstance(m, nn.Linear)][-1].out_features\n",
    "        self.metric = Accuracy(num_classes=num_classes, task=\"multiclass\")\n",
    "        self.opt_strat = opt_strat\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self.model(x)\n",
    "        loss = self.criterion(out, y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self.model(x)\n",
    "        self.log(\"val_acc\", self.metric(out, y), prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.opt_strat == \"step-160\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=0.1, weight_decay=1e-4, momentum=0.9)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 120], gamma=0.1)\n",
    "        elif self.opt_strat == \"step-200\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=1e-1, weight_decay=5e-4, momentum=0.9)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2)\n",
    "        elif self.opt_strat == \"cos-200\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=0.1, weight_decay=5e-4, momentum=0.9)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "    \n",
    "\n",
    "def train(model: nn.Module, dataset: str=\"cifar100\", opt_strat=\"step160\", batch_size=64, seed=0):\n",
    "    L.seed_everything(seed, workers=True)\n",
    "    datamodule = get_datamodule(dataset, batch_size=batch_size)\n",
    "    module = LitModel(model, opt_strat=opt_strat)\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=int(opt_strat.split(\"-\")[-1]),\n",
    "        enable_checkpointing=False,\n",
    "        enable_model_summary=False,\n",
    "        logger=None,\n",
    "        precision=\"16-mixed\",\n",
    "        deterministic=True,\n",
    "    )\n",
    "    trainer.fit(module, datamodule=datamodule)\n",
    "    return trainer.logged_metrics[\"val_acc\"].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vs Akamaster\n",
    "- Yerlan Idelbayev's implementation seems to be the standard reference\n",
    "- Comparing by:\n",
    "    - making sure both networks are initialised using the same random seed, and then can produce the same output given the same input\n",
    "    - training loop to get same perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import httpimport\n",
    "import pandas as pd\n",
    "\n",
    "def get_resnet_akamaster(model: str, num_classes: int = 10):\n",
    "    \"\"\"CIFAR-native models\"\"\"\n",
    "    with httpimport.github_repo(\"akamaster\", \"pytorch_resnet_cifar10\"):\n",
    "        import resnet as resnet_akamaster\n",
    "        model = resnet_akamaster.__dict__[model]()\n",
    "        model.linear = nn.Linear(model.linear.in_features, num_classes)\n",
    "        return model\n",
    "    \n",
    "akamaster_resnet = get_resnet_akamaster(\"resnet56\")\n",
    "torch.manual_seed(0)\n",
    "akamaster_resnet = akamaster_resnet.apply(init_weights)\n",
    "\n",
    "my_resnet = get_resnet(\"resnet56\", 10)\n",
    "torch.manual_seed(0)\n",
    "my_resnet = my_resnet.apply(init_weights)\n",
    "\n",
    "X = torch.randn(1, 3, 32, 32)\n",
    "print(\"Outputs match:\", torch.allclose(akamaster_resnet(X), my_resnet(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>akamaster</th>\n",
       "      <th>mine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7202</td>\n",
       "      <td>0.7198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   akamaster    mine\n",
       "0     0.7202  0.7198"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "akamaster_resnet = get_resnet_akamaster(\"resnet56\", num_classes=100)\n",
    "my_resnet = get_resnet(\"resnet56\", num_classes=100)\n",
    "res = {\n",
    "    \"akamaster\": train(akamaster_resnet, dataset=\"cifar100\", opt_strat=\"step-200\"), \n",
    "    \"mine\": train(my_resnet, dataset=\"cifar100\", opt_strat=\"step-200\")\n",
    "}\n",
    "pd.DataFrame(res, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Benchmarks\n",
    "- Major difference between Imagenet and CIFAR versions on perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = []\n",
    "for dataset in [\"cifar10\", \"cifar100\"]:\n",
    "    for model in [\"resnet18\", \"resnet20\", \"resnet32\", \"resnet34\", \"resnet44\", \"resnet50\", \"resnet56\"]:\n",
    "        net = get_resnet(model, num_classes=10 if dataset == \"cifar10\" else 100)\n",
    "        acc = train(net, dataset, opt_strat=\"step-200\", batch_size=64)\n",
    "        benchmarks.append({\"dataset\": dataset, \"model\": model, \"val_acc\": acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>cifar10</th>\n",
       "      <th>cifar100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>resnet18</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet20</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet32</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet34</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet44</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet50</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet56</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset   cifar10  cifar100\n",
       "model                      \n",
       "resnet18     0.95      0.78\n",
       "resnet20     0.92      0.68\n",
       "resnet32     0.93      0.69\n",
       "resnet34     0.96      0.79\n",
       "resnet44     0.92      0.70\n",
       "resnet50     0.95      0.78\n",
       "resnet56     0.92      0.71"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step-200 bs=64\n",
    "pd.DataFrame(benchmarks).pivot(index=\"model\", columns=\"dataset\", values=\"val_acc\").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>cifar10</th>\n",
       "      <th>cifar100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>resnet18</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet20</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet32</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet34</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet44</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet50</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet56</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset   cifar10  cifar100\n",
       "model                      \n",
       "resnet18     0.92      0.71\n",
       "resnet20     0.91      0.67\n",
       "resnet32     0.91      0.67\n",
       "resnet34     0.92      0.67\n",
       "resnet44     0.92      0.68\n",
       "resnet50     0.91      0.65\n",
       "resnet56     0.89      0.68"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step-160, bs 256\n",
    "pd.DataFrame(benchmarks).pivot(index=\"model\", columns=\"dataset\", values=\"val_acc\").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HP Benchmarks\n",
    "- Optimisation Strategy and Batch Size hyper-param analysis\n",
    "- BS major impact on step-16 perf; 200 epoch optimisations a lot less sensitive to batch-size\n",
    "- Only bs=64 epoch=200 versions can get closer to 73% (seed dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cifar100\"\n",
    "model = \"resnet56\"\n",
    "model_benchmarks = []\n",
    "for batch_size in [64, 128, 256]:\n",
    "    for opt_strat in [\"step-160\", \"step-200\", \"cos-200\"]:\n",
    "        net = get_resnet(model, num_classes=10 if dataset == \"cifar10\" else 100)\n",
    "        acc = train(net, dataset, opt_strat=opt_strat, batch_size=batch_size)\n",
    "        model_benchmarks.append({\"dataset\": dataset, \"model\": model, \"val_acc\": acc, \"opt_strat\": opt_strat, \"batch_size\": batch_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>batch_size</th>\n",
       "      <th>64</th>\n",
       "      <th>128</th>\n",
       "      <th>256</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opt_strat</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cos-200</th>\n",
       "      <td>0.7272</td>\n",
       "      <td>0.7162</td>\n",
       "      <td>0.7147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step-160</th>\n",
       "      <td>0.6997</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.6740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step-200</th>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.7107</td>\n",
       "      <td>0.7117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "batch_size     64      128     256\n",
       "opt_strat                         \n",
       "cos-200     0.7272  0.7162  0.7147\n",
       "step-160    0.6997  0.6861  0.6740\n",
       "step-200    0.7127  0.7107  0.7117"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_benchmarks).pivot(index=\"opt_strat\", columns=\"batch_size\", values=\"val_acc\").round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd]",
   "language": "python",
   "name": "conda-env-phd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
