{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "atomic-saudi",
   "metadata": {},
   "source": [
    "# Pytorch Distributed Data Parallel\n",
    "- Playing around with functionality, addressing each topic of interest separately\n",
    "- By topic of interest I mean things that either werent clear to me or pain points when using ddp in my workflow\n",
    "- Some essential things are not that straight forward in ddp: validation, logging, returning stuff\n",
    "- Running DDP in Jupyter is not easy. The workaround I'm using is to run the program in a separate process altogether. For this I have to write all imports and helper functions to a .py file. Sounds complicated but it can be neatly done with cell magics. \n",
    "- Note: DDP works by running the entire spawned function in separate processes, each with access to a different GPU. The only way we can control what each process executes is through the `rank` variable, which is passed as the first argument. Model parameters are read-only shared between processes. Processes are synced under the hood when .backward is triggered. We can sync and communicate between procs manually using torch.distributed.\n",
    "\n",
    "### Contents:   \n",
    "0. Jupyter DDP Cell Magic: to elegantly run DDP in Jupyter  \n",
    "1. Helper functions: functions used in all other examples\n",
    "2. Quick Train: to test if bare ddp works  \n",
    "3. Sampling: How distributed sampling works, looking at indices  \n",
    "4. Synchronisation: Looking at the timestamps of processes with or without sync  \n",
    "5. Validation and Logging: How to validate using ddp and informally handle logs  \n",
    "6. Passing Args: Passing parameters through the spawner without being awkward  \n",
    "7. Checkpointing and Resuming: Saving and Loading DDP models workaround  \n",
    "8. Returns: Being able to receive ddp process returns  \n",
    "9. Summary: Sample script for ddp training with some generic functions solving above issues  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "current-ecuador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "print(\"Available GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-horizontal",
   "metadata": {},
   "source": [
    "# 0. Jupyter DDP Cell Magic\n",
    "- Jupyter doesnt like DDP, breaks if run directly in a cell or even with %run\n",
    "- Need to write the executable code to a file and run it in a separate process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attractive-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def prun(line, cell):\n",
    "    \"\"\"\n",
    "    Run a cell as a different python process and print outputs.\n",
    "    Ipykernel messes with concurrency so running multiproc in jupyter can be tricky.\n",
    "    Saves the cell as a temp file and runs python on it with popen.\n",
    "    Note: the data streams printed out are sorted based on process, not actual time.\n",
    "    :param line: path to where to create temp file, defaults to /tmp\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(dir=line) as tmp:\n",
    "        tmp.write(cell.encode())\n",
    "        tmp.flush()\n",
    "        process = subprocess.Popen([\"python\", tmp.name], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        while True:\n",
    "            line = process.stdout.readline().decode(\"utf-8\")\n",
    "            if line != \"\":\n",
    "                print(line)\n",
    "            if process.poll() is not None:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-christianity",
   "metadata": {},
   "source": [
    "# 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "threaded-portfolio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_ddp.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import vgg11_bn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "\n",
    "def get_loader(rank=0, batch_size=128, train=True, distributed=True):\n",
    "    norm = T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "    transform = T.Compose([            \n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            norm,\n",
    "    ]) if train else T.Compose([T.ToTensor(), norm])\n",
    "    ds = CIFAR10(root=\"/data/datasets/CIFAR10\", train=train, transform=transform)\n",
    "    sampler = DistributedSampler(ds, num_replicas=2, rank=rank) if distributed else None\n",
    "    dataloader = DataLoader(ds, batch_size=batch_size, sampler=sampler)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def get_model(rank):\n",
    "    model = torch.nn.Sequential(*vgg11_bn().features, nn.Flatten(), nn.Linear(512, 10))\n",
    "    model = model.to(rank)\n",
    "    model = DDP(model, device_ids=[rank], output_device=rank)\n",
    "    return model\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def run_ddp(trainer, *args):\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(trainer, args=args, nprocs=world_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-layout",
   "metadata": {},
   "source": [
    "# 2. Quick Test Train\n",
    "- Just to see if it works\n",
    "- Print the device of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ranking-testament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on device: cuda:1\n",
      "\n",
      "Model on device: cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%prun .\n",
    "\n",
    "from my_ddp import *\n",
    "\n",
    "\n",
    "def train(rank, epochs=2):\n",
    "    setup(rank, world_size=torch.cuda.device_count())\n",
    "    model = get_model(rank)\n",
    "    train_loader = get_loader(rank, train=True, distributed=True)\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "    print(\"Model on device:\", model.module[0].weight.device)\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(rank), y.to(rank)\n",
    "            out = model(X)\n",
    "            loss = nn.functional.cross_entropy(out, y)\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_ddp(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-prospect",
   "metadata": {},
   "source": [
    "# 3. Sampling\n",
    "- Visualise the sampling of the dataset using DistributedSampler\n",
    "- Use a dummy tensordataset with 20 data points and print their indices\n",
    "- Without the distributed sampler each process just goes over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "confidential-series",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With distributed Sampling\n",
      "\n",
      "Rank: 1, Batch: 0, Indices:[ 0  2  5  6  8 12 14 17 18 19]\n",
      "\n",
      "Rank: 0, Batch: 0, Indices:[ 1  3  4  7  9 10 11 13 15 16]\n",
      "\n",
      "\n",
      "\n",
      "Without distributed Sampling\n",
      "\n",
      "Rank: 1, Batch: 0, Indices:[0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "Rank: 1, Batch: 1, Indices:[10 11 12 13 14 15 16 17 18 19]\n",
      "\n",
      "Rank: 0, Batch: 0, Indices:[0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "Rank: 0, Batch: 1, Indices:[10 11 12 13 14 15 16 17 18 19]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%prun .\n",
    "\n",
    "from my_ddp import *\n",
    "\n",
    "\n",
    "def train(rank, dist_sampling=True):\n",
    "    setup(rank, world_size=torch.cuda.device_count())\n",
    "    \n",
    "    ds = TensorDataset(torch.tensor(range(20)))\n",
    "    sampler = DistributedSampler(ds, num_replicas=2, rank=rank) if dist_sampling else None\n",
    "    loader = DataLoader(ds, batch_size=10, sampler=sampler)\n",
    "    \n",
    "    for batch, (X, ) in enumerate(loader):\n",
    "        print(f\"Rank: {rank}, Batch: {batch}, Indices:{X.sort()[0].numpy()}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"With distributed Sampling\")\n",
    "    run_ddp(train, True)\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"Without distributed Sampling\")\n",
    "    run_ddp(train, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-richmond",
   "metadata": {},
   "source": [
    "# 4. Synchronisation\n",
    "- For some reason the processes run sequentially if no sync trigger?\n",
    "- Both dist.barrier or loss.backward() trigger syncs.\n",
    "- Backward doesnt seem to work without zero_grad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "authentic-header",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Sync\n",
      "\n",
      "Time: 03:44, Rank: 0, Batch: 0\n",
      "\n",
      "Time: 03:45, Rank: 0, Batch: 1\n",
      "\n",
      "Time: 03:47, Rank: 0, Batch: 2\n",
      "\n",
      "Time: 03:44, Rank: 1, Batch: 0\n",
      "\n",
      "Time: 03:45, Rank: 1, Batch: 1\n",
      "\n",
      "Time: 03:47, Rank: 1, Batch: 2\n",
      "\n",
      "\n",
      "\n",
      "Without Sync\n",
      "\n",
      "Time: 03:54, Rank: 0, Batch: 0\n",
      "\n",
      "Time: 03:54, Rank: 0, Batch: 1\n",
      "\n",
      "Time: 03:55, Rank: 0, Batch: 2\n",
      "\n",
      "Time: 03:53, Rank: 1, Batch: 0\n",
      "\n",
      "Time: 03:56, Rank: 1, Batch: 1\n",
      "\n",
      "Time: 03:58, Rank: 1, Batch: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%prun .\n",
    "\n",
    "from my_ddp import *\n",
    "\n",
    "\n",
    "def train(rank, sync=True):\n",
    "    setup(rank, world_size=torch.cuda.device_count())\n",
    "    model = get_model(rank)\n",
    "    \n",
    "    for batch in range(3):\n",
    "        model.zero_grad()\n",
    "        time.sleep(torch.randn(1).abs().item()*2)\n",
    "        if sync:\n",
    "            model(torch.randn(1, 3, 32, 32).to(rank)).sum().backward()  # option 1\n",
    "            #dist.barrier()  # option 2\n",
    "        print(f\"Time: {datetime.now().strftime('%M:%S')}, Rank: {rank}, Batch: {batch}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"With Sync\")\n",
    "    run_ddp(train, True)\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"Without Sync\")\n",
    "    run_ddp(train, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-provider",
   "metadata": {},
   "source": [
    "# 5. Validation and Logging\n",
    "- More concerned with logging validation scores rather than random stuff\n",
    "- Standard practice seems to validate only on one process, while the other ones are idle :-/\n",
    "- For larger datasets we would ideally have a dsitributed validation and aggregate scores\n",
    "- The workflow would be to use a distributed sampler during validation, and reducing the accuracy to only one process wich \n",
    "- Note: Reduce only reduces on the destination process, the other ones keep the value of their tensor unchanged\n",
    "- Note: The default operation for reduce is SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sufficient-edition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43:39, Rank: 1, Partial: 6\n",
      "\n",
      "Time: 43:40, Rank: 1, Full: 6\n",
      "\n",
      "Time: 43:36, Rank: 0, Partial: 2\n",
      "\n",
      "Time: 43:41, Rank: 0, Full: 8\n",
      "\n",
      "Time: 43:41, Rank: 0, This gets logged: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%prun .\n",
    "\n",
    "from my_ddp import *\n",
    "\n",
    "# TOY EXAMPLE\n",
    "def train(rank):\n",
    "    setup(rank, world_size=torch.cuda.device_count())\n",
    "    \n",
    "    time.sleep(torch.randn(1).abs().item()*2)\n",
    "    score = torch.randint(0, 10, (1,)).to(rank)\n",
    "    print(f\"Time: {datetime.now().strftime('%M:%S')}, Rank: {rank}, Partial: {score.item()}\")\n",
    "\n",
    "    dist.reduce(score, dst=0)\n",
    "\n",
    "    time.sleep(torch.randn(1).abs().item()*2)\n",
    "    print(f\"Time: {datetime.now().strftime('%M:%S')}, Rank: {rank}, Full: {score.item()}\")\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"Time: {datetime.now().strftime('%M:%S')}, Rank: {rank}, This gets logged: {score.item()}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_ddp(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "collaborative-onion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1, n_correct: 4145.0, n_obs: 5000.0\n",
      "\n",
      "Rank: 0, n_correct: 4094.0, n_obs: 5000.0\n",
      "\n",
      "Final Acc -  n_correct: 8239.0, n_obs: 10000.0, acc: 0.8238999843597412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%prun .\n",
    "\n",
    "from my_ddp import *\n",
    "\n",
    "# ACTUAL TRAINING AND VALIDATING\n",
    "def train(rank):\n",
    "    setup(rank, world_size=torch.cuda.device_count())\n",
    "    model = get_model(rank)\n",
    "\n",
    "    train_loader = get_loader(rank, train=True, distributed=True)\n",
    "    val_loader = get_loader(rank, train=False, distributed=True)\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "    for epoch in range(10):\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(rank), y.to(rank)\n",
    "            out = model(X)\n",
    "            loss = nn.functional.cross_entropy(out, y)\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "    # logging only once at the end to keep things clean\n",
    "    model.eval()\n",
    "    \n",
    "    n_correct = torch.tensor(0.).to(rank)\n",
    "    n_obs = torch.tensor(0.).to(rank)\n",
    "    for X, y in val_loader:\n",
    "        X, y = X.to(rank), y.to(rank)\n",
    "        with torch.no_grad():\n",
    "            out = model(X)\n",
    "            n_correct +=(out.argmax(dim=-1) == y).float().sum()\n",
    "            n_obs += len(X)\n",
    "            \n",
    "    print(f\"Rank: {rank}, n_correct: {n_correct}, n_obs: {n_obs}\")\n",
    "            \n",
    "    dist.reduce(n_correct, dst=0)\n",
    "    dist.reduce(n_obs, dst=0)\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"Final Acc -  n_correct: {n_correct}, n_obs: {n_obs}, acc: {n_correct / n_obs}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_ddp(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-vegetable",
   "metadata": {},
   "source": [
    "# 6. Passing Args\n",
    "- Unfortunately spawn accepts only one \"args\" parameter, which passes the arguments of the executable function\n",
    "- This means only positional arguments, not named ones can be used - which is muy not elegante for my workflow at least\n",
    "- This is where some studio engineering comes in handy my hard rocking amigo\n",
    "- Basically we need a wrapper function that splits the \"args\" into positinoal and named parameters. Args we pass to spwan will be a tuple of a list and a dict.\n",
    "- This way we can still define a trainer and call the runner like a normal person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "mexican-karma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The defaults are: a=1, b=2, c=3, d=4\n",
      "\n",
      "Passed args are: '6, 7, d=9'\n",
      "\n",
      "Rank: 1, a=6, b=7, c=3, d=9\n",
      "\n",
      "Rank: 0, a=6, b=7, c=3, d=9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%prun .\n",
    "\n",
    "from my_ddp import *\n",
    "\n",
    "\n",
    "def run_ddp(*args, **kwargs):\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train_wrapper, args=(args, kwargs), nprocs=world_size)\n",
    "\n",
    "    \n",
    "def train_wrapper(rank, args: list = None, kwargs: dict = None):\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "    train(rank, *args, **kwargs)\n",
    "    \n",
    "\n",
    "def train(rank, a=1, b=2, c=3, d=4):\n",
    "    setup(rank, world_size=torch.cuda.device_count())\n",
    "    print(f\"Rank: {rank}, a={a}, b={b}, c={c}, d={d}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"The defaults are: a=1, b=2, c=3, d=4\")\n",
    "    print(\"Passed args are: '6, 7, d=9'\")\n",
    "    run_ddp(6, 7, d=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-passion",
   "metadata": {},
   "source": [
    "# 7. Checkpointing and Resuming\n",
    "- Official documentation suggests saving and loading the DDP model directly into GPU\n",
    "- I think it's just easier to work with the model.module and use regular save/load functions then re-DDP it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "impossible-acting",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, acc: 0.5824000239372253\n",
      "\n",
      "Epoch 1, acc: 0.6482999920845032\n",
      "\n",
      "Epoch 2, acc: 0.7166000008583069\n",
      "\n",
      "Resuming...\n",
      "\n",
      "Epoch 0, acc: 0.7473000288009644\n",
      "\n",
      "Epoch 1, acc: 0.7610999941825867\n",
      "\n",
      "Epoch 2, acc: 0.794700026512146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%prun .\n",
    "\n",
    "from my_ddp import *\n",
    "\n",
    "\n",
    "def evaluate(rank, model, loader):\n",
    "    model.eval()\n",
    "    n_correct = torch.tensor(0.).to(rank)\n",
    "    n_obs = torch.tensor(0.).to(rank)\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(rank), y.to(rank)\n",
    "        with torch.no_grad():\n",
    "            out = model(X)\n",
    "            n_correct +=(out.argmax(dim=-1) == y).float().sum()\n",
    "            n_obs += len(X)\n",
    "            \n",
    "    #print(f\"Rank: {rank}, partial acc: {n_correct / n_obs}\")\n",
    "    dist.reduce(n_correct, dst=0)\n",
    "    dist.reduce(n_obs, dst=0)\n",
    "    \n",
    "    acc = (n_correct / n_obs).detach().item()\n",
    "    model.train()\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train(rank, resume=False):\n",
    "    setup(rank, world_size=torch.cuda.device_count())\n",
    "    \n",
    "    if resume:\n",
    "        model = torch.load(\"model.pt\")\n",
    "    else:\n",
    "        model = torch.nn.Sequential(*vgg11_bn().features, nn.Flatten(), nn.Linear(512, 10))\n",
    "        \n",
    "    model = model.to(rank)\n",
    "    model = DDP(model, device_ids=[rank], output_device=rank)\n",
    "\n",
    "    train_loader = get_loader(rank, train=True, distributed=True)\n",
    "    val_loader = get_loader(rank, train=False, distributed=True)\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "    for epoch in range(3):\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(rank), y.to(rank)\n",
    "            out = model(X)\n",
    "            loss = nn.functional.cross_entropy(out, y)\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "        val_acc = evaluate(rank, model, val_loader)\n",
    "    \n",
    "        if rank == 0:\n",
    "            print(f\"Epoch {epoch}, acc: {val_acc}\")\n",
    "            torch.save(model.module, \"model.pt\")\n",
    "        dist.barrier()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Train 3 epochs from scratch\n",
    "    run_ddp(train, False)\n",
    "    print(\"Resuming...\")\n",
    "    # Trainer another 3 epochs after resuming\n",
    "    run_ddp(train, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-seattle",
   "metadata": {},
   "source": [
    "# 8. Returns\n",
    "- I would like the trainer to return results e.g my logs\n",
    "- Spwan doesnt return anything. They could have done it a la joblib\n",
    "- To make things easy, I will just use process 0 for checkpointing and logging, so only its return is of interest\n",
    "- Similar to passing args I think the most elegant solution is to factor out that logic into wrappers\n",
    "- Will create a temp file and let process 0 serialise the return into it, will load from disk after ddp finishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "suitable-auckland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: {'whatever': [0]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%prun .\n",
    "\n",
    "import tempfile\n",
    "from my_ddp import *\n",
    "\n",
    "\n",
    "def run_ddp():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    with tempfile.NamedTemporaryFile() as tmp:\n",
    "        mp.spawn(train_wrapper, args=(tmp.name, ), nprocs=world_size)\n",
    "        res = torch.load(tmp.name)\n",
    "    return res\n",
    "\n",
    "    \n",
    "def train_wrapper(rank, tmp_path):\n",
    "    res = train(rank)\n",
    "    if rank == 0:\n",
    "        torch.save(res, tmp_path)\n",
    "        \n",
    "\n",
    "def train(rank):\n",
    "    setup(rank, world_size=torch.cuda.device_count())\n",
    "    return {\"whatever\": [rank]}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    res = run_ddp()\n",
    "    print(f\"Return: {res}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-separation",
   "metadata": {},
   "source": [
    "# 9. Summary\n",
    "- Putting it all together for some generic functions - run_ddp and proc_wrapper\n",
    "- They handle passing args and returning stuff\n",
    "- Using a sample trainer and train func to split model handling from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "traditional-asset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'val_acc': [0.5462999939918518, 0.6912999749183655, 0.7278000116348267, 0.7554000020027161, 0.7720999717712402]})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%prun .\n",
    "\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "from my_ddp import *\n",
    "\n",
    "\n",
    "def run_ddp(trainer, **kwargs):\n",
    "    \"\"\"\n",
    "    Use DDP to execute trainer in parallel, return trainer result from rank 0. \n",
    "    \"\"\"\n",
    "    world_size = torch.cuda.device_count()\n",
    "    with tempfile.NamedTemporaryFile() as tmp:\n",
    "        mp.spawn(proc_wrapper, args=(trainer, tmp.name, kwargs, ), nprocs=world_size)\n",
    "        res = torch.load(tmp.name)\n",
    "    return res\n",
    "\n",
    "    \n",
    "def proc_wrapper(rank, trainer, tmp_path: str, kwargs: dict = None):\n",
    "    \"\"\"\n",
    "    Wrapper to setup env, serialise trainer returns .\n",
    "    \"\"\"\n",
    "    setup(rank, world_size=torch.cuda.device_count())\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "        \n",
    "    res = trainer(rank, **kwargs)\n",
    "    if rank == 0:\n",
    "        torch.save(res, tmp_path)\n",
    "        \n",
    "\n",
    "def my_trainer(rank, epochs=10, lr=5e-4, batch_size=128, resume=False):\n",
    "    \"\"\"\n",
    "    Handles model saving/loading, passes to train func.\n",
    "    \"\"\"\n",
    "    if resume:\n",
    "        model = torch.load(\"model.pt\")\n",
    "    else:\n",
    "        model = torch.nn.Sequential(*vgg11_bn().features, nn.Flatten(), nn.Linear(512, 10))\n",
    "        \n",
    "    model = model.to(rank)\n",
    "    model = DDP(model, device_ids=[rank], output_device=rank)\n",
    "    model, logs = train(rank, model, epochs=epochs, lr=lr, batch_size=batch_size)\n",
    "    \n",
    "    if rank == 0:\n",
    "        torch.save(model.module, \"model.pt\")\n",
    "    return logs\n",
    "    \n",
    "    \n",
    "def train(rank, model, epochs=10, lr=5e-4, batch_size=128):\n",
    "    \"\"\"\n",
    "    Full training of model, returns logs of training.\n",
    "    \"\"\"\n",
    "    train_loader = get_loader(rank, batch_size=batch_size, train=True, distributed=True)\n",
    "    val_loader = get_loader(rank, train=False, distributed=True)      \n",
    "    \n",
    "    logs = defaultdict(list)\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(rank), y.to(rank)\n",
    "            out = model(X)\n",
    "            loss = nn.functional.cross_entropy(out, y)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        val_acc = evaluate(rank, model, val_loader) \n",
    "        logs[\"val_acc\"].append(val_acc)\n",
    "    return model, logs\n",
    "        \n",
    "\n",
    "def evaluate(rank, model, loader):\n",
    "    \"\"\"\n",
    "    Acc of model over given data loader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    n_correct = torch.tensor(0.).to(rank)\n",
    "    n_obs = torch.tensor(0.).to(rank)\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(rank), y.to(rank)\n",
    "        with torch.no_grad():\n",
    "            out = model(X)\n",
    "            n_correct += (out.argmax(dim=-1) == y).float().sum()\n",
    "            n_obs += len(X)\n",
    "            \n",
    "    #print(f\"Rank: {rank}, partial acc: {n_correct / n_obs}\")\n",
    "    dist.reduce(n_correct, dst=0)\n",
    "    dist.reduce(n_obs, dst=0)\n",
    "    \n",
    "    acc = (n_correct / n_obs).detach().item()\n",
    "    model.train()\n",
    "    return acc\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logs = run_ddp(my_trainer, epochs=5)\n",
    "    print(logs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev]",
   "language": "python",
   "name": "conda-env-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
