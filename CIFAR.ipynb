{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard CIFAR10/100 Setup\n",
    "Just a place to hold the standard models and hyperparams for cifar in one place so I don't have to keep looking up epochs and norm values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda import amp\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100, CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vgg16_bn, resnet50\n",
    "from torchmetrics import Accuracy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMVALS = {\n",
    "    \"mean\": {\n",
    "        \"cifar10\": [0.4914, 0.4822, 0.4465],\n",
    "        \"cifar100\": [0.5071, 0.4867, 0.4408],\n",
    "    },\n",
    "    \"std\": {\n",
    "        \"cifar10\": [0.2023, 0.1994, 0.2010],\n",
    "        \"cifar100\": [0.2675, 0.2565, 0.2761],\n",
    "    },\n",
    "}\n",
    "\n",
    "DATASET_ROOT = \"/data/datasets\"\n",
    "\n",
    "\n",
    "def get_transforms(dataset: str):\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(NORMVALS[\"mean\"][dataset], NORMVALS[\"std\"][dataset]),\n",
    "        ]\n",
    "    )\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(NORMVALS[\"mean\"][dataset], NORMVALS[\"std\"][dataset]),\n",
    "        ]\n",
    "    )\n",
    "    return train_transform, test_transform\n",
    "\n",
    "\n",
    "def get_loaders(dataset: str, batch_size=64):\n",
    "    train_transform, test_transform = get_transforms(dataset)\n",
    "    ds_cls = CIFAR10 if dataset == \"cifar10\" else CIFAR100\n",
    "    ds_path = DATASET_ROOT + \"/\" + dataset.upper()\n",
    "\n",
    "    train_ds = ds_cls(root=ds_path, transform=train_transform, train=True)\n",
    "    test_ds = ds_cls(root=ds_path, transform=test_transform, train=False)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- wrapped torchvision models - vgg16, resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dataset=\"cifar10\", model=\"vgg16\", init=True):\n",
    "    \"\"\"\n",
    "    Model factory that wraps classic torchvision models for cifar datasets\n",
    "    torchvision models are defined for imagenet and require slight adjustments to work on cifar.\n",
    "    When quickly prototyping one does not always want to define models from scratch.\n",
    "\n",
    "    As of now does not include the \"correct\" resnets for cifar e.g resnet32\n",
    "    \"\"\"\n",
    "\n",
    "    assert dataset in [\"cifar10\", \"cifar100\"]\n",
    "    assert model in [\"vgg16\", \"resnet50\"]\n",
    "    n_classes = 10 if dataset == \"cifar10\" else 100\n",
    "\n",
    "    if model == \"vgg16\":\n",
    "        model = vgg16_bn(pretrained=False)\n",
    "        model.features = model.features[:-1]  # dropping the last maxpool\n",
    "        model.avgpool = nn.AvgPool2d(kernel_size=2)\n",
    "        model.classifier = nn.Sequential(nn.Linear(512, n_classes))\n",
    "    if model == \"resnet50\":\n",
    "        model = resnet50(pretrained=False)\n",
    "        model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        model.relu = nn.Sequential()\n",
    "        model.maxpool = nn.Sequential()\n",
    "        repr_size = 512 * model.layer4[0].expansion  # resnet18/34: 512; resnet50:2048\n",
    "        model.fc = nn.Linear(in_features=repr_size, out_features=n_classes, bias=True)\n",
    "    if init:\n",
    "        model = init_model(model)\n",
    "    return model\n",
    "\n",
    "def init_model(model):\n",
    "    for m in [m for m in model.modules() if isinstance(m, nn.Conv2d)]:\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "\n",
    "    for m in [m for m in model.modules() if isinstance(m, nn.Linear)]:\n",
    "        m.weight.data.normal_(0, 0.01)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "    for m in [m for m in model.modules() if isinstance(m, nn.BatchNorm2d)]:\n",
    "        nn.init.constant_(m.weight, val=1.0)\n",
    "        nn.init.constant_(m.bias, val=0.0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "- Just test acc using torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def evaluate(loader, model) -> dict:\n",
    "    device = list(model.parameters())[0].device\n",
    "    n_classes = len(loader.dataset.classes)\n",
    "    metric = Accuracy(task=\"multiclass\", num_classes=n_classes).to(device)\n",
    "    model.eval()\n",
    "    for X, y in loader:\n",
    "        with amp.autocast():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            out = model(X)\n",
    "            metric.update(out, y)\n",
    "    model.train()\n",
    "    res = {\"acc\": metric.compute().item()}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(dataset=\"cifar10\", model=\"resnet50\", n_epochs=200, batch_size=64, device=\"cuda:0\"):\n",
    "    logs = defaultdict(list)\n",
    "    gscaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    model = get_model(dataset, model).to(device)\n",
    "    #model = torch.compile(model)\n",
    "\n",
    "    train_loader, test_loader = get_loaders(dataset, batch_size)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimiser = optim.SGD(model.parameters(), lr=1e-1, momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=n_epochs) # 200 epochs\n",
    "    #lr_scheduler = optim.lr_scheduler.MultiStepLR(optimiser, milestones=[80, 120], gamma=0.1) # 160 epochs\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device).long()\n",
    "                \n",
    "            with amp.autocast():\n",
    "                out = model(X)\n",
    "                loss = criterion(out, y)\n",
    "\n",
    "            gscaler.scale(loss).backward()\n",
    "            gscaler.step(optimiser)\n",
    "            gscaler.update()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "        logs[\"acc\"].append(evaluate(test_loader, model)[\"acc\"])\n",
    "        lr_scheduler.step()\n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Acc: 0.9476000070571899\n"
     ]
    }
   ],
   "source": [
    "res = trainer()\n",
    "print(f\"Best Acc: {max(res['acc'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev]",
   "language": "python",
   "name": "conda-env-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
