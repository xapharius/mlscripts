{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Mixup Keras\n",
    "- Keras implementation of manifold mixup (mixup on hidden layer outputs rather than raw inputs)\n",
    "- Keras makes it quite difficult to implement an elegant solution as it has quite a rigid structure\n",
    "- Basically do mixup on labels in the batch generation and pass on the sampled lambda to the network so it can do the mixup on the layer outputs later\n",
    "- If I wanted to do the mixup in one place I would have to create a custom loss layer that takes the mixed up labels instead of the ones supplied by the batch (-_-)\n",
    "- Havent tinkered around with hyperparams, at first glance mixup seems to hurt performance\n",
    "- TODO: tsne on embeddings w/o mixup to see if structure is indeed flat\n",
    "- TODO: test to see if combination actually is bug free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    " \n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    " \n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1 (Model)                  (None, 16, 16, 32)   10400       input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Mixup (Lambda)                  (None, 16, 16, 32)   0           block1[1][0]                     \n",
      "                                                                 block1[2][0]                     \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "block2 (Model)                  (None, 8, 8, 64)     55936       Mixup[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "final (Model)                   (None, 10)           40970       block2[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 107,306\n",
      "Trainable params: 106,922\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lambda_ = Input(shape=(1,))\n",
    "input_1 = Input(shape=x_train.shape[1:])\n",
    "input_2 = Input(shape=x_train.shape[1:])\n",
    "\n",
    "###############################\n",
    "\n",
    "input_ = Input(shape=x_train.shape[1:])\n",
    "\n",
    "block1 = Conv2D(32, (3,3), padding='same')(input_)\n",
    "block1 = Activation('elu')(block1)\n",
    "block1 = BatchNormalization()(block1)\n",
    "block1 = Conv2D(32, (3,3), padding='same')(block1)\n",
    "block1 = Activation('elu')(block1)\n",
    "block1 = BatchNormalization()(block1)\n",
    "block1 = MaxPooling2D()(block1)\n",
    "block1 = Dropout(0.2)(block1)\n",
    "\n",
    "block1 = Model(inputs=input_, outputs=block1, name=\"block1\")\n",
    "\n",
    "\n",
    "block1_out_1 = block1(input_1)\n",
    "block1_out_2 = block1(input_2)\n",
    "\n",
    "\n",
    "###############################\n",
    "# Manifold Mixup\n",
    "\n",
    "# lambda_ is conceputally a scalar, but had to encode it into a vector of same length as the batch otherwise keras will complain\n",
    "layer_mixup = keras.layers.Lambda(lambda inputs: inputs[2][0] * inputs[0] +  (1 - inputs[2][0]) * inputs[1], name=\"Mixup\")\n",
    "block1_out = layer_mixup([block1_out_1, block1_out_2, lambda_])\n",
    "\n",
    "\n",
    "###############################\n",
    "block2_in = Input(shape=block1_out.shape[1:].as_list())\n",
    "\n",
    "block2 = Conv2D(64, (3,3), padding='same')(block2_in)\n",
    "block2 = Activation('elu')(block2)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = Conv2D(64, (3,3), padding='same')(block2)\n",
    "block2 = Activation('elu')(block2)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = MaxPooling2D()(block2)\n",
    "block2 = Dropout(0.3)(block2)\n",
    "\n",
    "block2 = Model(inputs=block2_in, outputs=block2, name=\"block2\")\n",
    "block2_out = block2(block1_out)\n",
    " \n",
    "###############################\n",
    " \n",
    "final_in = Input(shape=block2_out.shape[1:].as_list())\n",
    "\n",
    "final = Flatten()(final_in)\n",
    "final = Dense(num_classes, activation='softmax')(final)\n",
    "\n",
    "final = Model(inputs=final_in, outputs=final, name=\"final\")\n",
    "final_out = final(block2_out)\n",
    "\n",
    "\n",
    "###############################\n",
    "\n",
    "\n",
    "model = Model(inputs=[input_1, input_2, lambda_], outputs=final_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_preproc(batch_X, batch_y, alpha=1., dummy=False):\n",
    "    if dummy:\n",
    "        lambda_ = np.ones(len(batch_X))\n",
    "        return [batch_X, batch_X, lambda_], batch_y\n",
    "    \n",
    "    indices = list(range(len(batch_X)))\n",
    "    np.random.shuffle(indices)  # shuffles inplace\n",
    "    \n",
    "    shuffled_X = batch_X[indices]\n",
    "    shuffled_y = batch_y[indices]\n",
    "    \n",
    "    \n",
    "    if alpha > 0:\n",
    "        lambda_ = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lambda_ = 1\n",
    "        \n",
    "    new_y = lambda_ * batch_y + (1 - lambda_) * shuffled_y\n",
    "    lambda_ = np.array([lambda_] * len(batch_X))\n",
    "    return [batch_X, shuffled_X, lambda_], new_y\n",
    "\n",
    "def mixup_preproc_gen(generator, dummy=False):\n",
    "    while True:\n",
    "        yield mixup_preproc(*next(generator), dummy=dummy)\n",
    "    \n",
    "\n",
    "def train(mixup: bool = False, n_epochs: int = 10):\n",
    "    batch_size = 64\n",
    "\n",
    "    #data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        )\n",
    "\n",
    "    datagen.fit(x_train)\n",
    "    gen = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "    gen_mixup = mixup_preproc_gen(gen, dummy=not mixup)\n",
    "\n",
    "    opt_rms = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "    model.fit_generator(gen_mixup,\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size, \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=1, \n",
    "                        validation_data=([x_test, x_test, np.ones(len(x_test))], y_test)\n",
    "                       )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.7624 - acc: 0.7375 - val_loss: 0.7678 - val_acc: 0.7428\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.7307 - acc: 0.7477 - val_loss: 0.8309 - val_acc: 0.7280\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.7135 - acc: 0.7535 - val_loss: 0.8328 - val_acc: 0.7294\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.6938 - acc: 0.7591 - val_loss: 0.6634 - val_acc: 0.7773\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.6810 - acc: 0.7651 - val_loss: 0.6550 - val_acc: 0.7844\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 18s 22ms/step - loss: 0.6607 - acc: 0.7703 - val_loss: 0.6808 - val_acc: 0.7755\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 18s 23ms/step - loss: 0.6512 - acc: 0.7760 - val_loss: 0.6718 - val_acc: 0.7805\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 18s 23ms/step - loss: 0.6416 - acc: 0.7779 - val_loss: 0.6218 - val_acc: 0.7914\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 0.6272 - acc: 0.7824 - val_loss: 0.6449 - val_acc: 0.7851\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6223 - acc: 0.7840 - val_loss: 0.6006 - val_acc: 0.7995\n",
      "10000/10000 [==============================] - 1s 82us/step\n",
      "\n",
      "Test result: 79.950 loss: 0.601\n"
     ]
    }
   ],
   "source": [
    "benchmark = train(mixup=False, n_epochs=10)\n",
    "scores = benchmark.evaluate([x_test, x_test, np.ones(len(x_test))], y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2932 - acc: 0.3308 - val_loss: 1.4860 - val_acc: 0.4937\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 1.7445 - acc: 0.4642 - val_loss: 1.0418 - val_acc: 0.6398\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 18s 23ms/step - loss: 1.5892 - acc: 0.5352 - val_loss: 0.9684 - val_acc: 0.6779\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 1.5150 - acc: 0.5697 - val_loss: 0.9683 - val_acc: 0.6766\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 1.4947 - acc: 0.5771 - val_loss: 0.9357 - val_acc: 0.6925\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 1.4493 - acc: 0.6028 - val_loss: 0.8393 - val_acc: 0.7208\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 18s 23ms/step - loss: 1.4323 - acc: 0.6097 - val_loss: 0.8140 - val_acc: 0.7395\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 18s 23ms/step - loss: 1.4158 - acc: 0.6165 - val_loss: 0.7808 - val_acc: 0.7431\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 18s 23ms/step - loss: 1.4001 - acc: 0.6264 - val_loss: 0.7589 - val_acc: 0.7506\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 18s 23ms/step - loss: 1.3765 - acc: 0.6380 - val_loss: 0.7635 - val_acc: 0.7536\n",
      "10000/10000 [==============================] - 1s 77us/step\n",
      "\n",
      "Test result: 75.360 loss: 0.764\n"
     ]
    }
   ],
   "source": [
    "mixup = train(mixup=True, n_epochs=10)\n",
    "scores = mixup.evaluate([x_test, x_test, np.ones(len(x_test))], y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
