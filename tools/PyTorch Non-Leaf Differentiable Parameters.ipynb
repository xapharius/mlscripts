{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "second-tennis",
   "metadata": {},
   "source": [
    "# PyTorch Non-Leaf Differentiable Parameters\n",
    "Keywords: \"non-leaf variables as module parameters\", \"backproping through a model\"\n",
    "\n",
    "## Motivation\n",
    "- In Meta-Learning we would like to learn how to train a network - we optimise for the optimisation algorithm itself.\n",
    "- In such a situation the loss is differentiated with respect to the learning algorithm rather than the model.\n",
    "- In other words the loss depends on how well the model is doing \"after\" training using that optimisation algorithm.\n",
    "- We have the meta-learner, which produces a model, and we optimise the meta-learner based on how well the trained model is doing.\n",
    "- MAML in particular does this by differentiating the loss of the trained model with respect to the meta-learner - one long backpropagation \"through\" the trained model.\n",
    "- While this sounds like an elegant approach, this is easier said than done on a practical level.\n",
    "- In this notebook we will examine the technical challenges and analyise possible solutions.\n",
    "\n",
    "\n",
    "## Problem Statement\n",
    "- Suppose we have two batches $B_1, B_2$, from a standard supervised problem, each with input and labels.\n",
    "- We start with a model $M_1$, which we update using SGD on $B_1$ to obtain $M_2$ \n",
    "$$M_2 = M_1 - \\nabla_{M_1} \\mathcal{L}(M_1, B_1)$$\n",
    "- Now we don't really care or want to keep $M_2$, we just want to use it to obtain an update on $M_1$ based on the performance of $M_2$ on $B_2$\n",
    "$$M_1* = M_1 - \\nabla_{M_1} \\mathcal{L}(M_2,B_2)$$\n",
    "- Unwrapping this, to emphasize that $M_2$ is just a temporary variable, we get:\n",
    "$$M_1* = M_1 - \\nabla_{M_1} \\mathcal{L}([M_1 - \\nabla_{M_1} \\mathcal{L}(M_1, B_1)], B_2)$$\n",
    "- However $M$ can be an arbitraty complicated neural network, so ideally we would use a `torch.nn.Module` to make use of the object-oriented paradigm and its ease of use. \n",
    "- This implies that we actually want to have the temporary network $M_2$.\n",
    "- Here is where the problem comes up, as we want to backprop \"through\" the `torch.nn.Module` $M_2$.\n",
    "- In our computational graph $M_2$ is not a leaf node, its parameters are somewhere in the middle between $M_1$ and $\\mathcal{L}(M_2, B_2)$.\n",
    "- Ideally this shouldn't be a problem, and we could just call backward on the outer loss to get the gradients for the $M_1$ model.\n",
    "- After all, autograd is powerful enough to produce higher order derivatives\n",
    "- The problem lies on a technical level - in Pytorch a model's weights are not seen as raw tensors but as `torch.nn.Parameters`, which are leaf nodes in the computational graph.\n",
    "- Wrapping tensors to `torch.nn.Parameters`, or using state_dict will copy the values and detach them from their computational graph.\n",
    "- These technical challenges are elaborated in the Higher paper. While theoretically a functional and OO implementation should be equivalent, in practice OO makes certain memory optimisations, such as updating variables in place, which prevent having a computational graph history for parameters.\n",
    "- We need to be able to create a module that keeps track of the computational graph, which lead to the current parameters.\n",
    "\n",
    "## Solutions\n",
    "- Obviously this is a big use-case so there are specialised libraries: higher and torch-meta.\n",
    "- In essense the proper way to do this is to use the functional equivalent for every operation. Functional is a pain to use for deep NNs, so it should happen behind the scenes while mainataining the OO api.\n",
    "- Another, but hacky, way to do this is to replace each parameter variable with a tensor. This is the way I'll implement a prototype here. \n",
    "\n",
    "Linkes with discussions:\n",
    "- https://discuss.pytorch.org/t/how-does-one-have-the-parameters-of-a-model-not-be-leafs/70076/6\n",
    "- https://discuss.pytorch.org/t/non-leaf-variables-as-a-modules-parameters/65775\n",
    "\n",
    "## Contents\n",
    "1. Walkthrough  \n",
    "    1.1 Using an optimiser  \n",
    "    1.2 Autograd  \n",
    "    1.3 Directly assigning tensors    \n",
    "    1.4 Wrapping weights as Parameters  \n",
    "    1.5 Tensor Hack  \n",
    "    1.6 Using Functional  \n",
    "2. Monkeypatching Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "related-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import copy\n",
    "import torch\n",
    "import tempfile\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-welcome",
   "metadata": {},
   "source": [
    "# 1. Walkthrough\n",
    "- We will test some naive approaches to understand the problem\n",
    "- Then we will go over attempts to tackle the non-leaf parameter problem.\n",
    "- We will also see what doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "empty-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_M1():\n",
    "    return nn.Linear(1, 1, bias=False)\n",
    "\n",
    "def get_random_loss(model):\n",
    "    \"\"\"Doesnt matter what this is as long as it can produce a gradient\"\"\"\n",
    "    in_ = torch.randn(1, 1)\n",
    "    out = model(in_)\n",
    "    loss = out.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-dressing",
   "metadata": {},
   "source": [
    "## 1.1 Using an optimiser\n",
    "- Using the SGD optimiser to update M1\n",
    "- This overwrites the parameters of M1, which becomes M2\n",
    "- We lose the computational graph that has the M1 weights as leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incredible-latter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"67pt\" height=\"38pt\"\n",
       " viewBox=\"0.00 0.00 67.00 38.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 34)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-34 63,-34 63,4 -4,4\"/>\n",
       "<!-- 140542513159200 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140542513159200</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"59,-30 0,-30 0,0 59,0 59,-30\"/>\n",
       "<text text-anchor=\"middle\" x=\"29.5\" y=\"-18\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">M1</text>\n",
       "<text text-anchor=\"middle\" x=\"29.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 1)</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fd29a921b20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = get_M1()\n",
    "sgd = SGD(M1.parameters(), lr=1)\n",
    "\n",
    "loss = get_random_loss(M1)\n",
    "loss.backward()\n",
    "sgd.step()\n",
    "\n",
    "make_dot(M1.weight, params={\"M1\": M1.weight})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-porter",
   "metadata": {},
   "source": [
    "# 1.2 Autograd\n",
    "- We definitely have to use autograd to keep the computational graph\n",
    "- Now the question is what do we do with the resulting tensor M2_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "iraqi-runner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"109pt\" height=\"214pt\"\n",
       " viewBox=\"0.00 0.00 109.00 214.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 210)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-210 105,-210 105,4 -4,4\"/>\n",
       "<!-- 140547034508000 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140547034508000</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"80,-30 21,-30 21,0 80,0 80,-30\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-18\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">M2</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 1)</text>\n",
       "</g>\n",
       "<!-- 140542503957600 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140542503957600</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"95,-85 6,-85 6,-66 95,-66 95,-85\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-73\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 140542503957600&#45;&gt;140547034508000 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140542503957600&#45;&gt;140547034508000</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-65.7796C50.5,-58.8654 50.5,-49.2417 50.5,-40.2296\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-40.1301 50.5,-30.1301 47.0001,-40.1302 54.0001,-40.1301\"/>\n",
       "</g>\n",
       "<!-- 140542503957024 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140542503957024</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-140 0,-140 0,-121 101,-121 101,-140\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-128\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140542503957024&#45;&gt;140542503957600 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140542503957024&#45;&gt;140542503957600</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-120.9197C50.5,-113.9083 50.5,-104.1442 50.5,-95.4652\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-95.3408 50.5,-85.3408 47.0001,-95.3409 54.0001,-95.3408\"/>\n",
       "</g>\n",
       "<!-- 140542513139120 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140542513139120</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"80,-206 21,-206 21,-176 80,-176 80,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-194\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">M1</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-183\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 1)</text>\n",
       "</g>\n",
       "<!-- 140542513139120&#45;&gt;140542503957024 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140542513139120&#45;&gt;140542503957024</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-175.7333C50.5,-168.0322 50.5,-158.5977 50.5,-150.3414\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-150.0864 50.5,-140.0864 47.0001,-150.0864 54.0001,-150.0864\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fd29a063b50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = get_M1()\n",
    "grad = torch.autograd.grad(get_random_loss(M1), M1.parameters(), retain_graph=True)[0]\n",
    "M2_weight = M1.weight - grad\n",
    "\n",
    "make_dot(M2_weight, params={\"M1\": M1.weight, \"M2\": M2_weight})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-score",
   "metadata": {},
   "source": [
    "## 1.3 Directly assigning tensors\n",
    "- As before we manually use autograd and compute the new weights\n",
    "- We create a new model and overwrite its weights with our computed ones, hoping the graph will stay in place\n",
    "- Pytorch is complaining the weights needs to be nn.Parameter not a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "structured-graduation",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-550499a227d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mM2_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mM2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mM2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM2_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/libs/miniconda3/envs/dev/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m                 raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                 \u001b[0;34m\"(torch.nn.Parameter or None expected)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m                                 .format(torch.typename(value), name))\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "M1 = get_M1()\n",
    "grad = torch.autograd.grad(get_random_loss(M1), M1.parameters(), retain_graph=True)[0]\n",
    "\n",
    "M2_weight = M1.weight - grad\n",
    "M2 = torch.nn.Linear(1, 1)\n",
    "M2.weight = M2_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-pennsylvania",
   "metadata": {},
   "source": [
    "## 1.4. Wrapping weights as Parameters\n",
    "- We saw that one can assign only nn.Parameters to model\n",
    "- Let's warp the weights as parameters then\n",
    "- We can see that once we do that, the compuational graph is discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "developmental-classification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"67pt\" height=\"38pt\"\n",
       " viewBox=\"0.00 0.00 67.00 38.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 34)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-34 63,-34 63,4 -4,4\"/>\n",
       "<!-- 140542504296096 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140542504296096</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"59,-30 0,-30 0,0 59,0 59,-30\"/>\n",
       "<text text-anchor=\"middle\" x=\"29.5\" y=\"-18\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">M2</text>\n",
       "<text text-anchor=\"middle\" x=\"29.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 1)</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fd29a0b9a00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = get_M1()\n",
    "grad = torch.autograd.grad(get_random_loss(M1), M1.parameters(), retain_graph=True)[0]\n",
    "\n",
    "M2_weight = nn.Parameter(M1.weight - grad)\n",
    "M2 = torch.nn.Linear(1, 1)\n",
    "M2.weight = M2_weight\n",
    "M2_loss = get_random_loss(M2)\n",
    "\n",
    "make_dot(M2.weight, params={\"M1\": M1.weight, \"M2\": M2.weight})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pretty-scheme",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ffa3b27a2ba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Trying to get the gradient of the M2 loss wrt to M1 fails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM2_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/libs/miniconda3/envs/dev/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "# Trying to get the gradient of the M2 loss wrt to M1 fails\n",
    "torch.autograd.grad(M2_loss, M1.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-companion",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.5 Tensor Hack\n",
    "- Based on https://discuss.pytorch.org/t/how-does-one-have-the-parameters-of-a-model-not-be-leafs/70076/6\n",
    "- mr alband suggests del the weight parameter, then assigning the non-leaf tensor\n",
    "- I would say this is more of a hack as it violates the intended use of model parameters, however if it works it works\n",
    "- One would need an automated way of monkey patching a whole network this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "requested-casino",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"109pt\" height=\"214pt\"\n",
       " viewBox=\"0.00 0.00 109.00 214.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 210)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-210 105,-210 105,4 -4,4\"/>\n",
       "<!-- 140542503956128 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140542503956128</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"80,-30 21,-30 21,0 80,0 80,-30\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-18\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">M2</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 1)</text>\n",
       "</g>\n",
       "<!-- 140542503965168 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140542503965168</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"95,-85 6,-85 6,-66 95,-66 95,-85\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-73\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 140542503965168&#45;&gt;140542503956128 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140542503965168&#45;&gt;140542503956128</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-65.7796C50.5,-58.8654 50.5,-49.2417 50.5,-40.2296\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-40.1301 50.5,-30.1301 47.0001,-40.1302 54.0001,-40.1301\"/>\n",
       "</g>\n",
       "<!-- 140542503965216 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140542503965216</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-140 0,-140 0,-121 101,-121 101,-140\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-128\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140542503965216&#45;&gt;140542503965168 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140542503965216&#45;&gt;140542503965168</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-120.9197C50.5,-113.9083 50.5,-104.1442 50.5,-95.4652\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-95.3408 50.5,-85.3408 47.0001,-95.3409 54.0001,-95.3408\"/>\n",
       "</g>\n",
       "<!-- 140542513159040 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140542513159040</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"80,-206 21,-206 21,-176 80,-176 80,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-194\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">M1</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-183\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 1)</text>\n",
       "</g>\n",
       "<!-- 140542513159040&#45;&gt;140542503965216 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140542513159040&#45;&gt;140542503965216</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-175.7333C50.5,-168.0322 50.5,-158.5977 50.5,-150.3414\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-150.0864 50.5,-140.0864 47.0001,-150.0864 54.0001,-150.0864\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fd29a0650d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = get_M1()\n",
    "grad = torch.autograd.grad(get_random_loss(M1), M1.parameters(), retain_graph=True)[0]\n",
    "\n",
    "M2_weight = M1.weight - grad\n",
    "M2 = torch.nn.Linear(1, 1)\n",
    "del M2.weight\n",
    "M2.weight = M2_weight\n",
    "M2_loss = get_random_loss(M2)\n",
    "\n",
    "make_dot(M2.weight, params={\"M1\": M1.weight, \"M2\": M2.weight})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "african-suite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1.weight has grad: True\n"
     ]
    }
   ],
   "source": [
    "M2_loss.backward()\n",
    "print(\"M1.weight has grad:\", M1.weight.grad is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-shakespeare",
   "metadata": {},
   "source": [
    "## 1.6 Using Functional\n",
    "- The non-leaf variable problem arises only when using the OO implementation of pytorch models, as nn.Parameter objects are designed to be leaf variables.\n",
    "- The functional api doesnt have that problem, however handling models as a nn.Modules is a lot more convenient than using a functinal implementation\n",
    "- It can become quite annoying to use this approach manually which boils down to either having two implementations of a model or wrap certain modules\n",
    "- However this seems to be the most legitimate solution and used by meta-learning libs \n",
    "- The key point is that one has to do this somehow under the hood, in a systematic way\n",
    "- In order to not interfere with the user experience and be completely transparent the solution has to provide a stateful, OO-like api, which under the hood actually uses the functional api \n",
    "- Higher monkey-patches modules at runtime such that their operations are converted to functional blocks, while torchmeta reimplements basic layers to explicity call their functional counterparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dominican-factor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1.weight has grad: True\n"
     ]
    }
   ],
   "source": [
    "M1 = get_M1()\n",
    "grad = torch.autograd.grad(get_random_loss(M1), M1.parameters(), retain_graph=True)[0]\n",
    "\n",
    "M2_weight = M1.weight - grad\n",
    "M2 = lambda x: torch.nn.functional.linear(x, weight=M2_weight)\n",
    "M2_loss = get_random_loss(M2)\n",
    "\n",
    "M2_loss.backward()\n",
    "print(\"M1.weight has grad:\", M1.weight.grad is not None)\n",
    "\n",
    "# Case in point about functional being cumbersome, I can't plot since M2 is not a nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-semiconductor",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Monkeypatching Prototype\n",
    "- Am going to use the tensor hack approach and monkey patch an arbitrary model's params\n",
    "- very crude approach, but is simple and works\n",
    "- Test by creating an arbitrary chain of vgg models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "isolated-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_differentiable_param(module, param_name: str, param: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Based on https://discuss.pytorch.org/t/how-does-one-have-the-parameters-of-a-model-not-be-leafs/70076/6\n",
    "    Deletes the Parameter and replaces it with a raw Tensor.\n",
    "    \"\"\"\n",
    "    if \".\" in param_name:\n",
    "        sub_module = getattr(module, param_name.split(\".\")[0])\n",
    "        suffix = \".\".join(param_name.split(\".\")[1:])\n",
    "        return assign_differentiable_param(sub_module, suffix, param)\n",
    "    delattr(module, param_name)\n",
    "    setattr(module, param_name, param)\n",
    "    \n",
    "\n",
    "def get_param(module, param_name: str):\n",
    "    if \".\" in param_name:\n",
    "        sub_module = getattr(module, param_name.split(\".\")[0])\n",
    "        suffix = \".\".join(param_name.split(\".\")[1:])\n",
    "        return get_param(sub_module, suffix)\n",
    "    return getattr(module, param_name)\n",
    "\n",
    "\n",
    "def create_differentiable_model(base_model, params: dict = None):\n",
    "    \"\"\"\n",
    "    Creata a model with differentiable parameters from a base model.\n",
    "    If params is not None, assign those parameters, otherwise make old ones differentiable.\n",
    "    Also works if base_model is aready differentiable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        diff_model = copy.deepcopy(base_model)\n",
    "    except RuntimeError:\n",
    "        # Can't copy a non-leaf parameter model\n",
    "        # https://stackoverflow.com/questions/56590886/how-to-solve-the-run-time-error-only-tensors-created-explicitly-by-the-user-gr\n",
    "        with tempfile.NamedTemporaryFile() as tmp:\n",
    "            # Need dill because of the lambdas\n",
    "            torch.save(base_model, tmp.name, pickle_module=dill)\n",
    "            diff_model = torch.load(tmp.name, pickle_module=dill)\n",
    "    \n",
    "    if params is None:\n",
    "        params = {param_name: param + 0 for param_name, param in base_model.named_parameters()}\n",
    "    \n",
    "    for param_name, param in params.items():\n",
    "        assign_differentiable_param(diff_model, param_name, param)\n",
    "    \n",
    "    # Note: by using this hacky monkey patching the new model doesnt have parametes anymore, only tensors. \n",
    "    # This means the get_parameters() won't return anything. We need to patch that, further going down the rabbit hole.\n",
    "    diff_model.parameter_names = list(params.keys())\n",
    "    diff_model.named_parameters = lambda: {name: get_param(diff_model, name) for name in diff_model.parameter_names}.items()\n",
    "    diff_model.parameters = lambda: [get_param(diff_model, name) for name in diff_model.parameter_names]\n",
    "    return diff_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-filling",
   "metadata": {},
   "source": [
    "## 2.1 Testing on VGG\n",
    "- Using VGG as it has a bunch of different layer types\n",
    "- Unrolling it 3 times: Base -> M1 -> M2 -> M3\n",
    "- Get grads of loss M3 wrt to Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "right-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg11_bn\n",
    "\n",
    "base_model = vgg11_bn()\n",
    "diff_model = base_model\n",
    "\n",
    "batch_size = 5\n",
    "get_batch = lambda: (torch.randn(batch_size, 3, 32, 32), torch.randint(high=1000, size=(batch_size,)))\n",
    "\n",
    "\n",
    "for _ in range(3):\n",
    "    X, y = get_batch()\n",
    "    loss = nn.functional.cross_entropy(diff_model(X), y)\n",
    "    grads = torch.autograd.grad(loss, diff_model.parameters(), retain_graph=True)\n",
    "    new_params = {key: param - grad for (key, param), grad in zip(diff_model.named_parameters(), grads)}\n",
    "    diff_model = create_differentiable_model(diff_model, new_params)\n",
    "    \n",
    "X, y = get_batch()\n",
    "loss = nn.functional.cross_entropy(diff_model(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hungarian-scanning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model grads are None: True\n"
     ]
    }
   ],
   "source": [
    "print(\"base_model grads are None:\", base_model.features[0].weight.grad is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "clear-exception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model grads are NOT None now: True\n"
     ]
    }
   ],
   "source": [
    "loss.backward() # loss of M3\n",
    "print(\"base_model grads are NOT None now:\", base_model.features[0].weight.grad is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-format",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- `nn.Parameters` are by design leaf nodes in the computational graph\n",
    "- Overwriting them naively doesn't work: `nn.Module`s require `nn.Parameters` not tensors\n",
    "- Specialised libraries use `nn.functional` under the hood\n",
    "- A hacky way to monkey-patch is to force a `nn.Module` to use raw tensors (del parameters)\n",
    "    - Implemented a working prototype of such a wrapper that works for arbitrary architectures and multiple inner loops"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev]",
   "language": "python",
   "name": "conda-env-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
