{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Stitch Networks\n",
    "- Paper: Cross-Stitch Networks for Multi-task Learning - Misra 2016 CVPR\n",
    "- Reference pytorch implementations: [Vandenhende](https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch/blob/master/models/cross_stitch.py); [MTAN](https://github.com/lorenmt/mtan/blob/master/im2im_pred/model_segnet_cross.py)\n",
    "    - Existing implementions seem unecessarily complicated or tightly coupled, hence this notebook\n",
    "- Sime idea - keep a separate backbone per task and regularly combine activations (linearly)\n",
    "    - Backbones are not task-specific - through cross-stitching they are jointly optimised\n",
    "    - The way to control how much of the backbone remains task-specific is through the mixing parameters (alpha, beta)\n",
    "    - Mixing parameters are initialised (and thought of) as a convex combination, however there are no contraints during training making it effectively a linear combination\n",
    "    - Alpha dictates how much weight to give the own backbone, while Beta how much weights to attribute to other task's backbones (activations)\n",
    "    - During the forward pass the input has to pass through all backbones simulatneously and be cross-stitched\n",
    "\n",
    "![](https://d3i71xaburhd42.cloudfront.net/2976605dc3b73377696537291d45f09f1ab1fbf5/3-Figure3-1.png)\n",
    "- Downsides:\n",
    "    - Number of parameters increases linearly with number of tasks - we continue to add backbones (a lot)\n",
    "    - Works only in single-input multi-output settings (not multi-domain)\n",
    "    - Because of the high parameter count, it might not be possible to fit in memory if the backbone is large or we have many tasks\n",
    "\n",
    "![](https://d3i71xaburhd42.cloudfront.net/2976605dc3b73377696537291d45f09f1ab1fbf5/4-Figure4-1.png)\n",
    "\n",
    "- Implementation notes:\n",
    "    - Authors suggest to put Cross-Stitch units after pooling layers, in resnets the equivalent would be after each stage (when downsampling)\n",
    "    - Cross-stitching is meant to be applied per channel. A \"unit\" refers one [n_tasks x n_tasks] matrix for one channel. \n",
    "        - Although there was one case in the paper where it is applied per layer. MTAN impl does this.\n",
    "    - Unclear what the suggested initialisation for CrossStitch units is, paper has ablations but does not specify what the default should be\n",
    "        - Vadenhende uses (a=0.9, b=0.1), while Liu uses (a=1,b=1)\n",
    "    - Backbone initialisation - paper prefers single-task pretraining. For a fair comparison with related work, the backbones should be intialised identically. In practice the backbones are pre-trained networks.\n",
    "    \n",
    "- Tests\n",
    "    - Testing against Vadenhende implementation for resnet backbones. Needed to modify to decouple from rest of library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone CrossStitch Implementation\n",
    "- Backbone/encoder logic only\n",
    "\n",
    "- Minimal Implementation\n",
    "    - Only for resnets, very similar to reference\n",
    "\n",
    "- Generic Implementation\n",
    "    - works for resnets/vggs\n",
    "    - has a bit of bloat to determine stages and channel sizes\n",
    "    - Factory method to instantiate common resnet/vgg arch based on str\n",
    "        - patches up vggs to have \"stage\" structure\n",
    "        - identical task backbones\n",
    "\n",
    "- Use cases:\n",
    "    - DeepLabs - Encapsulate and add heads\n",
    "    - SegNet: Extend to patch maxpools and override forward to handle indices; encapsulate and add decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class CrossStitchLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Keeps all mixing weights in a single tensor. Dict input and output.\n",
    "    :author: @xapharius\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tasks: list, num_channels: int, alpha=0.9, beta=0.1):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.task_idx = {t: idx for idx, t in enumerate(tasks)}\n",
    "\n",
    "        # units[task1][task2] returns the weigths that should be applied on the channels of task2's backbone when computing the output of task1\n",
    "        self.units = nn.Parameter(torch.Tensor(len(tasks), len(tasks), num_channels))\n",
    "        self.init(alpha, beta)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"CrossStitchLayer(tasks={list(self.task_idx.keys())}, num_channels={self.num_channels}\"\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"allows for [ti, tj] indexing\"\"\"\n",
    "        ti, tj = item\n",
    "        return self.units[self.task_idx[ti], self.task_idx[tj]]\n",
    "\n",
    "    def init(self, alpha: float, beta: float):\n",
    "        self.units.data.fill_(beta)\n",
    "        n_tasks = len(self.task_idx)\n",
    "        self.units.data[range(n_tasks), range(n_tasks), :] = alpha\n",
    "        return self\n",
    "\n",
    "    def forward(self, x: dict) -> dict:\n",
    "        return {\n",
    "            ti: torch.sum(\n",
    "                torch.stack([self[ti, tj].view(1, -1, 1, 1) * x[tj] for tj in x]),\n",
    "                dim=0,\n",
    "            )\n",
    "            for ti in x\n",
    "        }\n",
    "\n",
    "\n",
    "class CrossStitchResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Resnets only, requires passing num channels for each stage. \n",
    "    Generic implementation is lengthier but has more helper functions.\n",
    "    \"\"\"\n",
    "    STAGES = [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]\n",
    "\n",
    "    def __init__(self, backbones: dict, channels: list, alpha=0.9, beta=0.1):\n",
    "        super().__init__()\n",
    "        self.backbones = nn.ModuleDict(backbones)\n",
    "        self.tasks = list(backbones.keys())\n",
    "\n",
    "        cs = {stage: CrossStitchLayer(self.tasks, c_, alpha, beta) for stage, c_ in zip(self.STAGES, channels)}\n",
    "        self.cross_stitch = nn.ModuleDict(cs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> dict:\n",
    "        x = {\n",
    "            task: m.maxpool(m.relu(m.bn1(m.conv1(x))))\n",
    "            for task, m in self.backbones.items()\n",
    "        }\n",
    "\n",
    "        for stage in self.STAGES:\n",
    "            for task in self.tasks:\n",
    "                x[task] = getattr(self.backbones[task], stage)(x[task])\n",
    "            x = self.cross_stitch[stage](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# cs = CrossStitchLayer([\"task1\", \"task2\"], 2)\n",
    "# cs({\"task1\": torch.ones(1, 2), \"task2\": -torch.ones(1, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericCrossStitchBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Only the backbone/encoder logic, extend or encapsulate to add heads.\n",
    "    For resnet type models with names stages \"layerX\". VGGs can be patched to have the same structure.\n",
    "    Has factories, and figures out the number of channels in each stage automatically.\n",
    "    :author: @xapharius\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbones: dict, alpha=0.9, beta=0.1):\n",
    "        \"\"\"\n",
    "        :param backbones: {task: resnet}\n",
    "        :param alpha, beta: same/other task weight\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.backbones = nn.ModuleDict(backbones)\n",
    "        self.tasks = list(backbones.keys())\n",
    "\n",
    "        _bb = backbones[self.tasks[0]]\n",
    "        self.stages = [f\"layer{i}\" for i in range(1, 6) if hasattr(_bb, f\"layer{i}\")]\n",
    "        \n",
    "        cs = dict()\n",
    "        for stage in self.stages:\n",
    "            channels = self.get_out_channels(getattr(_bb, stage))\n",
    "            cs[stage] = CrossStitchLayer(self.tasks, channels, alpha, beta)\n",
    "        self.cross_stitch = nn.ModuleDict(cs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> dict:\n",
    "        x = {task: x for task in self.tasks}\n",
    "\n",
    "        # Handle resnet pre-stage layers\n",
    "        if hasattr(self.backbones[self.tasks[0]], \"conv1\"):\n",
    "            x = {\n",
    "                task: m.maxpool(m.relu(m.bn1(m.conv1(x[task]))))\n",
    "                for task, m in self.backbones.items()\n",
    "            }\n",
    "\n",
    "        for stage in self.stages:\n",
    "            for task in self.tasks:\n",
    "                x[task] = getattr(self.backbones[task], stage)(x[task])\n",
    "            x = self.cross_stitch[stage](x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def get_out_channels(model: nn.Module):\n",
    "        \"\"\"Based on the last conv or batchnorm layer in the model\"\"\"\n",
    "        m = [m for m in model.modules() if isinstance(m, (nn.Conv2d, nn.BatchNorm2d))][-1]\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            return m.out_channels\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            return m.num_features\n",
    "\n",
    "    @staticmethod\n",
    "    def patch_vgg(model: nn.Module) -> nn.Module:\n",
    "        \"\"\"Split vgg layers into stages similar to resnets\"\"\"\n",
    "        maxpool_idx = [ix for ix, m in enumerate(model.features) if isinstance(m, nn.MaxPool2d)]\n",
    "        return nn.Sequential(\n",
    "            OrderedDict({\n",
    "                f\"layer{i+1}\": nn.Sequential(*model.features[ix_start + 1 : ix_end + 1])\n",
    "                for i, (ix_start, ix_end) in enumerate(\n",
    "                    zip([-1] + maxpool_idx, maxpool_idx)\n",
    "                )\n",
    "            })\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def factory(\n",
    "        cls, tasks: list, model=\"resnet18\", pretrained=None, alpha=0.9, beta=0.1\n",
    "    ):\n",
    "        \"\"\"For predefined resnets/vggs, same init for each task\"\"\"\n",
    "        bb = getattr(torchvision.models, model)(weights=pretrained)\n",
    "        if \"vgg\" in model:\n",
    "            bb = cls.patch_vgg(bb)\n",
    "        backbones = {task: copy.deepcopy(bb) for task in tasks}\n",
    "        return cls(backbones, alpha, beta)\n",
    "\n",
    "\n",
    "# CrossStitchBackbone.factory(tasks=[\"t1\", \"t2\"], model=\"resnet18\", pretrained=True)\n",
    "# CrossStitchBackbone.factory(tasks=[\"t1\", \"t2\"], model=\"vgg16_bn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vandenhende Implementation\n",
    "https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch/blob/master/models/cross_stitch.py\n",
    "- Test against this implementation\n",
    "- Requires a bit of patching as it's tighly coupled to library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Authors: Simon Vandenhende\n",
    "# Licensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n",
    "\n",
    "\"\"\" \n",
    "    Implementation of cross-stitch networks\n",
    "    https://arxiv.org/abs/1604.03539\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ChannelWiseMultiply(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(ChannelWiseMultiply, self).__init__()\n",
    "        self.param = nn.Parameter(torch.FloatTensor(num_channels), requires_grad=True)\n",
    "\n",
    "    def init_value(self, value):\n",
    "        with torch.no_grad():\n",
    "            self.param.data.fill_(value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.mul(self.param.view(1,-1,1,1), x)\n",
    "\n",
    "\n",
    "class CrossStitchUnit(nn.Module):\n",
    "    def __init__(self, tasks, num_channels, alpha, beta):\n",
    "        super(CrossStitchUnit, self).__init__()\n",
    "        self.cross_stitch_unit = nn.ModuleDict({t: nn.ModuleDict({t: ChannelWiseMultiply(num_channels) for t in tasks}) for t in tasks})\n",
    "\n",
    "        for t_i in tasks:\n",
    "            for t_j in tasks:\n",
    "                if t_i == t_j:\n",
    "                    self.cross_stitch_unit[t_i][t_j].init_value(alpha)\n",
    "                else:\n",
    "                    self.cross_stitch_unit[t_i][t_j].init_value(beta)\n",
    "\n",
    "    def forward(self, task_features):\n",
    "        out = {}\n",
    "        for t_i in task_features.keys():\n",
    "            prod = torch.stack([self.cross_stitch_unit[t_i][t_j](task_features[t_j]) for t_j in task_features.keys()])\n",
    "            out[t_i] = torch.sum(prod, dim=0)\n",
    "        return out\n",
    "           \n",
    "\n",
    "class CrossStitchNetwork(nn.Module):\n",
    "    \"\"\" \n",
    "        Implementation of cross-stitch networks.\n",
    "        We insert a cross-stitch unit, to combine features from the task-specific backbones\n",
    "        after every stage.\n",
    "       \n",
    "        Argument: \n",
    "            backbone: \n",
    "                nn.ModuleDict object which contains pre-trained task-specific backbones.\n",
    "                {task: backbone for task in p.TASKS.NAMES}\n",
    "        \n",
    "            heads: \n",
    "                nn.ModuleDict object which contains the task-specific heads.\n",
    "                {task: head for task in p.TASKS.NAMES}\n",
    "        \n",
    "            stages: \n",
    "                list of stages where we instert a cross-stitch unit between the task-specific backbones.\n",
    "                Note: the backbone modules require a method 'forward_stage' to get feature representations\n",
    "                at the respective stages.\n",
    "        \n",
    "            channels: \n",
    "                dict which contains the number of channels in every stage\n",
    "        \n",
    "            alpha, beta: \n",
    "                floats for initializing cross-stitch units (see paper)\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, p, backbone: nn.ModuleDict, heads: nn.ModuleDict, \n",
    "                    stages: list, channels: dict, alpha: float, beta: float):\n",
    "        super(CrossStitchNetwork, self).__init__()\n",
    "\n",
    "        # Tasks, backbone and heads\n",
    "        # self.tasks = p.TASKS.NAMES\n",
    "        self.tasks = p # MY EDIT\n",
    "        self.backbone = backbone\n",
    "        self.heads = heads\n",
    "        self.stages = stages\n",
    "\n",
    "        # Cross-stitch units\n",
    "        self.cross_stitch = nn.ModuleDict({stage: CrossStitchUnit(self.tasks, channels[stage], alpha, beta) for stage in stages})\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        img_size = x.size()[-2:]\n",
    "        x = {task: x for task in self.tasks} # Feed as input to every single-task network\n",
    "\n",
    "        x = {task: m.maxpool(m.relu(m.bn1(m.conv1(x[task])))) for task, m in self.backbone.items()} # MY EDIT\n",
    "        \n",
    "        # Backbone\n",
    "        for stage in self.stages:\n",
    "    \n",
    "            # Forward through next stage of task-specific network\n",
    "            for task in self.tasks:\n",
    "                # x[task] = self.backbone[task].forward_stage(x[task], stage)\n",
    "                x[task] = getattr(self.backbone[task], stage)(x[task]) # MY EDIT\n",
    "            \n",
    "            # Cross-stitch the task-specific features\n",
    "            x = self.cross_stitch[stage](x)\n",
    "        return x\n",
    "        # Task-specific heads\n",
    "        # out = {task: self.heads[task](x[task]) for task in self.tasks}\n",
    "        # out = {task: F.interpolate(out[task], img_size, mode='bilinear') for task in self.tasks} \n",
    "\n",
    "        # return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference Implementation\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "p = [\"task1\", \"task2\"]\n",
    "stages = [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]\n",
    "channels = {'layer1': 64, 'layer2': 128, 'layer3': 256, 'layer4': 512}\n",
    "\n",
    "backbones = nn.ModuleDict({task: resnet18(weights=\"IMAGENET1K_V1\") for task in p})\n",
    "heads = nn.ModuleDict({task: nn.Sequential() for task in p})\n",
    "model_ref = CrossStitchNetwork(p, backbones, heads, stages, channels, alpha=0.9, beta=0.1)\n",
    "\n",
    "out_ref = model_ref(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching backbone outputs:\n",
      "task1 True\n",
      "task2 True\n"
     ]
    }
   ],
   "source": [
    "# Minimal ResNet CrossStitch\n",
    "\n",
    "backbones = nn.ModuleDict({task: resnet18(weights=\"IMAGENET1K_V1\") for task in p})\n",
    "channels = [64, 128, 256, 512]\n",
    "\n",
    "model_my = CrossStitchResNet(backbones, channels, alpha=0.9, beta=0.1)\n",
    "out_my = model_my(X)\n",
    "\n",
    "print(\"Matching backbone outputs:\")\n",
    "for task in out_ref.keys():\n",
    "    print(task, torch.equal(out_ref[task], out_my[task]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching backbone outputs:\n",
      "task1 True\n",
      "task2 True\n"
     ]
    }
   ],
   "source": [
    "# Generic CrossStitch\n",
    "\n",
    "model_my = GenericCrossStitchBackbone.factory(tasks=[\"task1\", \"task2\"], pretrained=\"IMAGENET1K_V1\", alpha=0.9, beta=0.1)\n",
    "out_my = model_my(X)\n",
    "\n",
    "print(\"Matching backbone outputs:\")\n",
    "for task in out_ref.keys():\n",
    "    print(task, torch.equal(out_ref[task], out_my[task]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev]",
   "language": "python",
   "name": "conda-env-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
